{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/duolacmeng/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# 设置环境变量\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4\"\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertGCN(nn.Module):\n",
    "    def __init__(self, num_vocab, bert_model_name='bert-base-uncased', gcn_hidden_dim=128, num_classes=2, num_words=5000, m=0.7):\n",
    "        super(BertGCN, self).__init__()\n",
    "        self.m = m\n",
    "        self.num_words = num_words\n",
    "        self.bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.gcn = GCNConv(1024, gcn_hidden_dim)  # 1024 是 BERT 的 hidden size\n",
    "        self.classifier_gcn = nn.Linear(gcn_hidden_dim, num_classes)\n",
    "        self.classifier_bert = nn.Linear(1024, num_classes)\n",
    "        self.word_embeddings = nn.Parameter(torch.zeros(num_vocab, 1024)) # 初始化词节点的特征为零向量\n",
    "\n",
    "    def forward(self, edge_index, edge_weight, input_ids, attention_mask, doc_indices, word_indices):\n",
    "        # 获取文档节点的BERT特征\n",
    "        bert_outputs = self.bert_model(input_ids, attention_mask)\n",
    "        doc_embeddings = bert_outputs.last_hidden_state[:, 0, :]  # [CLS] token 的嵌入表示\n",
    "\n",
    "        word_embeddings = self.word_embeddings.to(doc_embeddings.device)\n",
    "        \n",
    "        # GCN部分\n",
    "        combined_embeddings = torch.cat([doc_embeddings, word_embeddings], dim=0)\n",
    "        \n",
    "        gcn_embeddings = self.gcn(combined_embeddings, edge_index, edge_weight=edge_weight)\n",
    "        \n",
    "        # 仅提取文档节点的GCN特征\n",
    "        gcn_doc_embeddings = gcn_embeddings[:len(doc_indices)]\n",
    "        \n",
    "        # 分类器输出\n",
    "        gcn_logits = self.classifier_gcn(gcn_doc_embeddings)\n",
    "        gcn_probs = F.softmax(gcn_logits, dim=1)\n",
    "        \n",
    "        # BERT部分的预测（仅基于文档节点）\n",
    "        bert_logits = self.classifier_bert(doc_embeddings)\n",
    "        bert_probs = F.softmax(bert_logits, dim=1)\n",
    "        \n",
    "        # 平衡BERT和GCN的预测结果\n",
    "        final_probs = self.m * gcn_probs + (1 - self.m) * bert_probs\n",
    "        return final_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = '/root/Mcqueen/kaggle_c/MyBertGCN/data/train.csv'\n",
    "bert_model_name='/root/Mcqueen/kaggle_c/model/tuned_deberta_v3large/output_8/checkpoint-980'\n",
    "epochs=20\n",
    "batch_size=2\n",
    "lr=1e-3\n",
    "m=0.7\n",
    "\n",
    "\n",
    "# 加载图数据\n",
    "adj = sp.load_npz('./graph/adj.npz')\n",
    "features = sp.load_npz('./graph/features.npz').todense()\n",
    "labels = torch.load('./graph/labels.pt').to(device) - 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj.tocoo()  # 将csr格式存储的稀疏矩阵转换成coo格式\n",
    "edge_index = torch.tensor(np.vstack((adj.row, adj.col)), dtype=torch.long).to(device)\n",
    "edge_weight = torch.tensor(adj.data, dtype=torch.float).to(device)\n",
    "\n",
    "# 读取数据集\n",
    "df = pd.read_csv(data_path)\n",
    "num_docs = len(df)\n",
    "num_words = adj.shape[0] - num_docs - 1 # 词汇表的大小\n",
    "doc_indices = torch.arange(num_docs, dtype=torch.long).to(device)  + 1\n",
    "word_indices = torch.arange(num_words, dtype=torch.long).to(device) + num_docs + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5910356])\n",
      "[0.04388138 0.04347523 0.05358129 0.0647622  0.05047925 0.04459067\n",
      " 0.05354018 0.05184324 0.04772471 0.06099653 0.01646671 0.04941215\n",
      " 0.05743867 0.01493714 0.04330815 0.0381083  0.10602278 0.0164276\n",
      " 0.13424944 0.05489831]\n",
      "17307\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(edge_index.shape)\n",
    "print(adj.data[:20])  # 这里的data是稀疏矩阵的值，实际上是节点之间的similariy\n",
    "\n",
    "print(num_docs)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/duolacmeng/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/Mcqueen/kaggle_c/pretrained_llm/deberta-v3-large\")\n",
    "all_encodings = tokenizer(df['full_text'].values.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "# train_encodings = tokenizer(df['full_text'].values[train_indices].tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "# val_encodings = tokenizer(df['full_text'].values[val_indices].tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "for key in all_encodings.keys():\n",
    "    all_encodings[key] = all_encodings[key].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, edge_index, edge_weight, y, input_ids, attention_mask, doc_indices, word_indices):\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_weight = edge_weight\n",
    "        self.y = y\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.doc_indices = doc_indices\n",
    "        self.word_indices = word_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            if idx.start is None or idx.stop is None:\n",
    "                raise ValueError(\"slice start and stop cannot be None\")\n",
    "            step = idx.step if idx.step is not None else 1\n",
    "            idx = list(range(idx.start, idx.stop, step))\n",
    "        idx = idx.tolist()\n",
    "        print(\"idx\",idx)\n",
    "        input_ids = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "        doc_idx = [item+1 for item in idx]  # 因为文章都排在前面\n",
    "\n",
    "        # 创建index到老index的映射\n",
    "        selected_docs = doc_idx\n",
    "        all_indices = selected_docs + self.word_indices.tolist()\n",
    "        idx_map = {old_idx: new_idx + 1 for new_idx, old_idx in enumerate(all_indices)}\n",
    "\n",
    "        # 处理边，使得不应该存在的边无效，有效的边进行端点的重新映射\n",
    "\n",
    "        num_docs = len(self.doc_indices)\n",
    "\n",
    "        print(num_docs)\n",
    "\n",
    "        new_edge_index = self.edge_index.clone()\n",
    "        new_edge_index[new_edge_index > num_docs] -= (num_docs - len(idx))  # 处理word节点\n",
    "        # 处理doc节点\n",
    "        temp = new_edge_index[self.edge_index <= num_docs].cpu().numpy()\n",
    "        mapped_indices = np.vectorize(lambda x: idx_map.get(x, -1))(temp)\n",
    "        new_edge_index[self.edge_index <= num_docs] = torch.tensor(mapped_indices, device=new_edge_index.device)\n",
    "        mask = (new_edge_index == -1).any(dim=0)\n",
    "        filtered_new_edge_index = new_edge_index[:, ~mask]\n",
    "\n",
    "        # 更新edge_weight\n",
    "        filtered_edge_weight = self.edge_weight[~mask]  # 要传递梯度\n",
    "        \n",
    "        # 获取新的doc和word索引\n",
    "        new_doc_indices = torch.tensor([idx_map[i] for i in doc_idx], dtype=torch.long)\n",
    "        new_word_indices = torch.tensor([idx_map[i.item()] for i in self.word_indices], dtype=torch.long)\n",
    "\n",
    "        return CustomDataset(filtered_new_edge_index, filtered_edge_weight, self.y[doc_idx], input_ids, attention_mask, new_doc_indices, new_word_indices)\n",
    "\n",
    "\n",
    "\n",
    "data_all = CustomDataset(edge_index, edge_weight, labels, all_encodings['input_ids'], all_encodings['attention_mask'], doc_indices, word_indices)\n",
    "train_indices, val_indices = train_test_split(np.arange(num_docs), test_size=0.1)\n",
    "print(train_indices)\n",
    "\n",
    "data_train = data_all[train_indices]\n",
    "data_val = data_all[val_indices]\n",
    "\n",
    "\n",
    "print(edge_weight.device)\n",
    "print(all_encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(data_all))\n",
    "print(edge_index[:, :10])\n",
    "# print(data_all.edge_index[:, :100])\n",
    "# print((data_all.edge_index==1).sum())\n",
    "print(data_all[[0]].edge_index[:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "formatted_time = time.strftime(\"%Y%m%H%M%S\", time.localtime())\n",
    "\n",
    "writer = SummaryWriter(f'runs/{formatted_time}')\n",
    "\n",
    "def qwk(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "\n",
    "def train(model, data_train=data_train, data_eval=data_val, batch_size=2, eval_batch_size=4, num_epochs=5, lr=0.001):\n",
    "    global_step = 0\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f'Epoch {epoch} begin ...')\n",
    "        total_loss = 0.0\n",
    "        epoch_step = 0\n",
    "        all_pred = []\n",
    "        idx_list = list(range(len(data_train)))\n",
    "        random.shuffle(idx_list)\n",
    "        for i in range(0, len(idx_list), batch_size):\n",
    "            epoch_step += 1\n",
    "            global_step += 1\n",
    "            if i + batch_size < len(idx_list):\n",
    "                idxs = idx_list[i: i+batch_size]\n",
    "            else:\n",
    "                idxs = idx_list[i:]\n",
    "            optimizer.zero_grad()\n",
    "            batch = data_train[idxs]\n",
    "            input_ids, attention_mask, doc_indices, edge_index, edge_weight, word_indices, labels = batch.input_ids, batch.attention_mask, batch.doc_indices, batch.edge_index, batch.edge_weight, batch.word_indices, batch.y\n",
    "            out = model(edge_index, edge_weight, input_ids, attention_mask, doc_indices, word_indices)\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_pred += pred.tolist()\n",
    "            loss = F.cross_entropy(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('Loss/Train', loss.item(), global_step)\n",
    "            print('new iteration')\n",
    "            total_loss += loss.item()\n",
    "            if epoch_step % 1 == 0 or epoch_step == 1:\n",
    "                print(f'    Epoch {epoch} Iteration {epoch_step} Avg train loss: {(total_loss/epoch_step):.3f}')\n",
    "        print(f'Epoch {epoch} Train loss: {(total_loss / (len(idx_list)//batch_size + 1)):.3f}\\n')\n",
    "        epoch_qwk = qwk(data_train.y.tolist(), all_pred)\n",
    "        print(f'QWK score: {epoch_qwk:.3f}')\n",
    "        writer.add_scalar('QWK/Train', epoch_qwk, epoch)\n",
    "    print\n",
    "    evaluate(model, data_eval=data_eval, batch_size=eval_batch_size, curr_epoch=epoch)\n",
    "\n",
    "\n",
    "def evaluate(model, data_eval, batch_size, curr_epoch=None):\n",
    "    print('\\nEvaluate ...')\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    idx_list = list(range(len(data_eval)))\n",
    "    random.shuffle(idx_list)\n",
    "    all_pred = []\n",
    "    eval_step = 0\n",
    "    for i in range(0, len(idx_list), batch_size):\n",
    "        eval_step += 1\n",
    "        if i + batch_size < len(idx_list):\n",
    "            idxs = idx_list[i: i+batch_size]\n",
    "        else:\n",
    "            idxs = idx_list[i:]\n",
    "        batch = data_train[idxs]\n",
    "        input_ids, attention_mask, doc_indices, edge_index, edge_weight, word_indices, labels = batch.input_ids, batch.attention_mask, batch.doc_indices, batch.edge_index, batch.edge_weight, batch.word_indices, batch.y\n",
    "        with torch.no_grad():\n",
    "            out = model(edge_index, edge_weight, input_ids, attention_mask, doc_indices, word_indices)\n",
    "            loss = F.cross_entropy(out, labels)\n",
    "            total_loss += loss.item()\n",
    "            pred = out.argmax(dim=1)\n",
    "            all_pred += pred.tolist()\n",
    "            correct = (pred == batch.y).sum().item()\n",
    "            total_correct += correct\n",
    "            if eval_step % 100 == 0 or eval_step == 1:\n",
    "                print(f'Avg eval loss: {(total_loss/eval_step):.3f}, Avg eval acc: {(total_correct/(batch_size*eval_step)):.3f}')\n",
    "    avg_loss = total_loss / (len(idx_list)//batch_size + 1)\n",
    "    accuracy = total_correct / len(idx_list)\n",
    "    eval_qwk = qwk(data_eval.y.tolist(), all_pred)\n",
    "    print(data_eval.y.tolist(), all_pred)\n",
    "    print(f'QWK score: {eval_qwk:.3f}')\n",
    "    if curr_epoch:\n",
    "        writer.add_scalar('Loss/Val', avg_loss, curr_epoch)\n",
    "        writer.add_scalar('Accuracy/Val', accuracy, curr_epoch)\n",
    "        writer.add_scalar('QWK/Val', eval_qwk, curr_epoch)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 439270284 (1675.68 MB)\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model = BertGCN(num_vocab=num_words, bert_model_name=bert_model_name, num_classes=len(set(labels.tolist())), m=m, num_words=num_words)\n",
    "model = model.to(device)\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 打印优化器参数名称和形状\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"Parameter name: {name}, shape: {param.shape}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total_params} ({total_params*4/(1024*1024):.2f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "train(model=model, data_train=data_train, batch_size=2, num_epochs=5, lr=0.0003)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate ...\n",
      "Avg eval loss: 1.550, Avg eval acc: 0.500\n",
      "[2, 1, 2, 1, 2, 2, 1, 2, 2, 1] [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "QWK score: 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.4546708861986797, 0.3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model=model, data_eval=data_val[0:10], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:3',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.word_embeddings[300:310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwk([4, 1, 1, 4, 3, 2, 1, 1, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
