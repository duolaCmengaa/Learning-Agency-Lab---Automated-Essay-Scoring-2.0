{"cells":[{"cell_type":"markdown","metadata":{},"source":["本实验的路径为kaggle上的虚拟目录。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:37:21.619168Z","iopub.status.busy":"2024-04-29T08:37:21.618767Z","iopub.status.idle":"2024-04-29T08:38:05.465088Z","shell.execute_reply":"2024-04-29T08:38:05.464163Z","shell.execute_reply.started":"2024-04-29T08:37:21.619130Z"},"trusted":true},"outputs":[],"source":["import gc\n","import torch\n","import copy\n","from datasets import Dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,DataCollatorWithPadding\n","import nltk\n","from datasets import Dataset\n","from glob import glob\n","import numpy as np \n","import pandas as pd\n","import polars as pl\n","import re\n","import random\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","from scipy.special import softmax\n","from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\n","from sklearn.linear_model import LogisticRegression, Perceptron\n","from sklearn.svm import LinearSVC\n","from sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB\n","from sklearn.neural_network import MLPClassifier\n","from sklearn import tree\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","from imblearn.ensemble import BalancedBaggingClassifier\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.datasets import make_classification\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n","from sklearn.metrics import cohen_kappa_score\n","from lightgbm import log_evaluation, early_stopping\n","import lightgbm as lgb\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:38:05.467826Z","iopub.status.busy":"2024-04-29T08:38:05.467097Z","iopub.status.idle":"2024-04-29T08:38:05.473298Z","shell.execute_reply":"2024-04-29T08:38:05.471373Z","shell.execute_reply.started":"2024-04-29T08:38:05.467791Z"},"trusted":true},"outputs":[],"source":["MAX_LENGTH = 1024\n","TEST_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\n","MODEL_PATH = '/kaggle/input/aes2-400-20240419134941/*/*'\n","EVAL_BATCH_SIZE = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:38:05.474679Z","iopub.status.busy":"2024-04-29T08:38:05.474413Z","iopub.status.idle":"2024-04-29T08:39:31.668157Z","shell.execute_reply":"2024-04-29T08:39:31.667046Z","shell.execute_reply.started":"2024-04-29T08:38:05.474656Z"},"trusted":true},"outputs":[],"source":["models = glob(MODEL_PATH)\n","tokenizer = AutoTokenizer.from_pretrained(models[0])\n","\n","def tokenize(sample):\n","    return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n","\n","df_test = pd.read_csv(TEST_DATA_PATH)\n","ds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n","\n","args = TrainingArguments(\n","    \".\", \n","    per_device_eval_batch_size=EVAL_BATCH_SIZE, \n","    report_to=\"none\"\n",")\n","\n","predictions = []\n","for model in models:\n","    model = AutoModelForSequenceClassification.from_pretrained(model)\n","    trainer = Trainer(\n","        model=model, \n","        args=args, \n","        data_collator=DataCollatorWithPadding(tokenizer), \n","        tokenizer=tokenizer\n","    )    \n","    preds = trainer.predict(ds).predictions\n","    predictions.append(softmax(preds, axis=-1))\n","    del model, trainer\n","    torch.cuda.empty_cache()\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:39:31.671988Z","iopub.status.busy":"2024-04-29T08:39:31.671116Z","iopub.status.idle":"2024-04-29T08:39:31.676761Z","shell.execute_reply":"2024-04-29T08:39:31.675831Z","shell.execute_reply.started":"2024-04-29T08:39:31.671949Z"},"trusted":true},"outputs":[],"source":["predicted_score = 0.\n","for p in predictions:\n","    predicted_score += p\n","    \n","predicted_score /= len(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:39:31.678361Z","iopub.status.busy":"2024-04-29T08:39:31.678018Z","iopub.status.idle":"2024-04-29T08:39:31.702107Z","shell.execute_reply":"2024-04-29T08:39:31.701343Z","shell.execute_reply.started":"2024-04-29T08:39:31.678336Z"},"trusted":true},"outputs":[],"source":["df_test['score'] = predicted_score.argmax(-1) + 1\n","df_test.head()"]},{"cell_type":"markdown","metadata":{},"source":["# **Data Loading**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:39:31.703661Z","iopub.status.busy":"2024-04-29T08:39:31.703315Z","iopub.status.idle":"2024-04-29T08:39:32.433402Z","shell.execute_reply":"2024-04-29T08:39:32.432463Z","shell.execute_reply.started":"2024-04-29T08:39:31.703630Z"},"trusted":true},"outputs":[],"source":["columns = [  \n","    (\n","        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n","    ),\n","]\n","PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n","\n","train = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\n","test = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n","\n","train.head(1)"]},{"cell_type":"markdown","metadata":{},"source":["# **Preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:39:32.435314Z","iopub.status.busy":"2024-04-29T08:39:32.435004Z","iopub.status.idle":"2024-04-29T08:39:32.458348Z","shell.execute_reply":"2024-04-29T08:39:32.457003Z","shell.execute_reply.started":"2024-04-29T08:39:32.435288Z"},"trusted":true},"outputs":[],"source":["cList = {\n","  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n","  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n","  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n","  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n","  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n","  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n","  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n","  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n","  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n","  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n","  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n","  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n","  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n","  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n","   }\n","\n","c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n","\n","def expandContractions(text, c_re=c_re):\n","    def replace(match):\n","        return cList[match.group(0)]\n","    return c_re.sub(replace, text)\n","\n","def removeHTML(x):\n","    html=re.compile(r'<.*?>')\n","    return html.sub(r'',x)\n","def dataPreprocessing(x):\n","    x = x.lower()\n","    x = removeHTML(x)\n","    x = re.sub(\"@\\w+\", '',x)\n","    x = re.sub(\"'\\d+\", '',x)\n","    x = re.sub(\"\\d+\", '',x)\n","    x = re.sub(\"http\\w+\", '',x)\n","    x = re.sub(r\"\\s+\", \" \", x)\n","#     x = expandContractions(x)\n","    x = re.sub(r\"\\.+\", \".\", x)\n","    x = re.sub(r\"\\,+\", \",\", x)\n","    x = x.strip()\n","    return x"]},{"cell_type":"markdown","metadata":{},"source":["# **Feature Engineering**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:39:32.460596Z","iopub.status.busy":"2024-04-29T08:39:32.459851Z","iopub.status.idle":"2024-04-29T08:39:34.929576Z","shell.execute_reply":"2024-04-29T08:39:34.928741Z","shell.execute_reply.started":"2024-04-29T08:39:32.460559Z"},"trusted":true},"outputs":[],"source":["import spacy\n","import re\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","with open('/kaggle/input/english-word-hx/words.txt', 'r') as file:\n","    english_vocab = set(word.strip().lower() for word in file)\n","    \n","def count_spelling_errors(text):\n","    doc = nlp(text)\n","    lemmatized_tokens = [token.lemma_.lower() for token in doc]\n","    spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n","    return spelling_errors"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:39:34.931078Z","iopub.status.busy":"2024-04-29T08:39:34.930779Z","iopub.status.idle":"2024-04-29T08:39:34.936226Z","shell.execute_reply":"2024-04-29T08:39:34.935230Z","shell.execute_reply.started":"2024-04-29T08:39:34.931048Z"},"trusted":true},"outputs":[],"source":["import string\n","def remove_punctuation(text):\n","    \"\"\"\n","    Remove all punctuation from the input text.\n","    \n","    Args:\n","    - text (str): The input text.\n","    \n","    Returns:\n","    - str: The text with punctuation removed.\n","    \"\"\"\n","\n","    translator = str.maketrans('', '', string.punctuation)\n","    return text.translate(translator)"]},{"cell_type":"markdown","metadata":{},"source":["### 1.Paragraph Features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:39:34.939760Z","iopub.status.busy":"2024-04-29T08:39:34.939485Z","iopub.status.idle":"2024-04-29T09:00:51.397576Z","shell.execute_reply":"2024-04-29T09:00:51.396606Z","shell.execute_reply.started":"2024-04-29T08:39:34.939728Z"},"trusted":true},"outputs":[],"source":["def Paragraph_Preprocess(tmp):\n","\n","    tmp = tmp.explode('paragraph')\n","    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n","    tmp = tmp.with_columns(pl.col('paragraph').map_elements(remove_punctuation).alias('paragraph_no_pinctuation'))\n","    tmp = tmp.with_columns(pl.col('paragraph_no_pinctuation').map_elements(count_spelling_errors).alias(\"paragraph_error_num\"))\n","    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n","    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n","                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n","    return tmp\n","\n","# feature_eng\n","paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n","paragraph_fea2 = ['paragraph_error_num'] + paragraph_fea\n","def Paragraph_Eng(train_tmp):\n","    num_list = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600]\n","    num_list2 = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700]\n","    aggs = [\n","        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_>{i}_cnt\") for i in [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n","        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_<{i}_cnt\") for i in [25,49]], \n","        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea2],\n","        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea2],\n","        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea2],\n","        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea2],\n","        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea2],\n","        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea2],\n","        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea2],\n","        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea2],\n","        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea2],\n","        ]\n","    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n","    df = df.to_pandas()\n","    return df\n","tmp = Paragraph_Preprocess(train)\n","train_feats = Paragraph_Eng(tmp)\n","train_feats['score'] = train['score']\n","\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n","print('Features Number: ',len(feature_names))\n","train_feats.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.Sentence Features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T09:00:51.399247Z","iopub.status.busy":"2024-04-29T09:00:51.398915Z","iopub.status.idle":"2024-04-29T09:00:58.480644Z","shell.execute_reply":"2024-04-29T09:00:58.479774Z","shell.execute_reply.started":"2024-04-29T09:00:51.399218Z"},"trusted":true},"outputs":[],"source":["def Sentence_Preprocess(tmp):\n","    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n","    tmp = tmp.explode('sentence')\n","    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n","    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))    \n","    return tmp\n","\n","# feature_eng\n","sentence_fea = ['sentence_len','sentence_word_cnt']\n","def Sentence_Eng(train_tmp):\n","    aggs = [\n","        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_>{i}_cnt\") for i in [0,15,50,100,150,200,250,300] ], \n","        *[pl.col('sentence').filter(pl.col('sentence_len') <= i).count().alias(f\"sentence_<{i}_cnt\") for i in [15,50] ], \n","        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n","        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n","        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n","        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n","        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n","        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n","        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n","        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea],\n","        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea],\n","    \n","        ]\n","    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n","    df = df.to_pandas()\n","    return df\n","\n","tmp = Sentence_Preprocess(train)\n","train_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n","\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n","print('Features Number: ',len(feature_names))\n","train_feats.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.Word Features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T09:00:58.482300Z","iopub.status.busy":"2024-04-29T09:00:58.482010Z","iopub.status.idle":"2024-04-29T09:01:11.859961Z","shell.execute_reply":"2024-04-29T09:01:11.858946Z","shell.execute_reply.started":"2024-04-29T09:00:58.482275Z"},"trusted":true},"outputs":[],"source":["# word feature\n","def Word_Preprocess(tmp):\n","    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n","    tmp = tmp.explode('word')\n","    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n","    tmp = tmp.filter(pl.col('word_len')!=0)    \n","    return tmp\n","\n","# feature_eng\n","def Word_Eng(train_tmp):\n","    aggs = [\n","        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n","        pl.col('word_len').max().alias(f\"word_len_max\"),\n","        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n","        pl.col('word_len').std().alias(f\"word_len_std\"),\n","        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n","        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n","        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n","        ]\n","    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n","    df = df.to_pandas()\n","    return df\n","\n","tmp = Word_Preprocess(train)\n","train_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n","\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n","print('Features Number: ',len(feature_names))\n","train_feats.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["### 4.Tf-idf features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T09:01:11.861288Z","iopub.status.busy":"2024-04-29T09:01:11.860994Z","iopub.status.idle":"2024-04-29T09:04:28.839332Z","shell.execute_reply":"2024-04-29T09:04:28.838337Z","shell.execute_reply.started":"2024-04-29T09:01:11.861264Z"},"trusted":true},"outputs":[],"source":["vectorizer = TfidfVectorizer(\n","            tokenizer=lambda x: x,\n","            preprocessor=lambda x: x,\n","            token_pattern=None,\n","            strip_accents='unicode',\n","            analyzer = 'word',\n","            ngram_range=(3,6),\n","            min_df=0.05,\n","            max_df=0.95,\n","            sublinear_tf=True,\n",")\n","\n","train_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n","dense_matrix = train_tfid.toarray()\n","df = pd.DataFrame(dense_matrix)\n","tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n","df.columns = tfid_columns\n","df['essay_id'] = train_feats['essay_id']\n","train_feats = train_feats.merge(df, on='essay_id', how='left')\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n","print('Number of Features: ',len(feature_names))\n","train_feats.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["### 5.CountVectorizer Features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T09:04:28.840863Z","iopub.status.busy":"2024-04-29T09:04:28.840573Z","iopub.status.idle":"2024-04-29T09:05:44.306180Z","shell.execute_reply":"2024-04-29T09:05:44.305289Z","shell.execute_reply.started":"2024-04-29T09:04:28.840839Z"},"trusted":true},"outputs":[],"source":["vectorizer_cnt = CountVectorizer(\n","            tokenizer=lambda x: x,\n","            preprocessor=lambda x: x,\n","            token_pattern=None,\n","            strip_accents='unicode',\n","            analyzer = 'word',\n","            ngram_range=(2,3),\n","            min_df=0.10,\n","            max_df=0.85,\n",")\n","train_tfid = vectorizer_cnt.fit_transform([i for i in train['full_text']])\n","dense_matrix = train_tfid.toarray()\n","df = pd.DataFrame(dense_matrix)\n","tfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\n","df.columns = tfid_columns\n","df['essay_id'] = train_feats['essay_id']\n","train_feats = train_feats.merge(df, on='essay_id', how='left')"]},{"cell_type":"markdown","metadata":{},"source":["### 6.Meta Features(deberta-v3-large)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T09:05:44.307868Z","iopub.status.busy":"2024-04-29T09:05:44.307566Z","iopub.status.idle":"2024-04-29T09:05:44.365895Z","shell.execute_reply":"2024-04-29T09:05:44.364950Z","shell.execute_reply.started":"2024-04-29T09:05:44.307841Z"},"trusted":true},"outputs":[],"source":["import joblib\n","\n","deberta_oof = joblib.load('/kaggle/input/aes2-400-20240419134941/oof.pkl')\n","print(deberta_oof.shape, train_feats.shape)\n","\n","for i in range(6):\n","    train_feats[f'deberta_oof_{i}'] = deberta_oof[:, i]\n","\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n","print('Features Number: ',len(feature_names))    \n","\n","train_feats.shape"]},{"cell_type":"markdown","metadata":{},"source":["# **Model training**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T09:05:44.367706Z","iopub.status.busy":"2024-04-29T09:05:44.367337Z","iopub.status.idle":"2024-04-29T09:05:44.375966Z","shell.execute_reply":"2024-04-29T09:05:44.374803Z","shell.execute_reply.started":"2024-04-29T09:05:44.367670Z"},"trusted":true},"outputs":[],"source":["def quadratic_weighted_kappa(y_true, y_pred):\n","    y_true = y_true + a\n","    y_pred = (y_pred + a).clip(1, 6).round()\n","    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n","    return 'QWK', qwk, True\n","def qwk_obj(y_true, y_pred):\n","    labels = y_true + a\n","    preds = y_pred + a\n","    preds = preds.clip(1, 6)\n","    f = 1/2*np.sum((preds-labels)**2)\n","    g = 1/2*np.sum((preds-a)**2+b)\n","    df = preds - labels\n","    dg = preds - a\n","    grad = (df/g - f*dg/g**2)*len(labels)\n","    hess = np.ones(len(labels))\n","    return grad, hess\n","a = 2.998\n","b = 1.092"]},{"cell_type":"markdown","metadata":{},"source":["# **FeatureSelection(Load High importance FE)**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T09:05:44.377415Z","iopub.status.busy":"2024-04-29T09:05:44.377106Z","iopub.status.idle":"2024-04-29T09:05:44.397949Z","shell.execute_reply":"2024-04-29T09:05:44.397153Z","shell.execute_reply.started":"2024-04-29T09:05:44.377390Z"},"trusted":true},"outputs":[],"source":["import pickle\n","with open('/kaggle/input/aes2-400-fes-202404291649/usefe_list.pkl', mode='br') as fi:\n","  feature_names = pickle.load(fi)\n","feature_select = feature_names\n","\n","\n","X = train_feats[feature_names].astype(np.float32).values\n","\n","y_split = train_feats['score'].astype(int).values\n","y = train_feats['score'].astype(np.float32).values-a\n","oof = train_feats['score'].astype(int).values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T09:05:45.561507Z","iopub.status.busy":"2024-04-29T09:05:45.561133Z","iopub.status.idle":"2024-04-29T09:05:45.567657Z","shell.execute_reply":"2024-04-29T09:05:45.566738Z","shell.execute_reply.started":"2024-04-29T09:05:45.561472Z"},"trusted":true},"outputs":[],"source":["len(feature_names)"]},{"cell_type":"markdown","metadata":{},"source":["# **Let's use cross-validation**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T09:05:45.569116Z","iopub.status.busy":"2024-04-29T09:05:45.568840Z"},"trusted":true},"outputs":[],"source":["n_splits = 15\n","\n","skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n","\n","f1_scores = []\n","kappa_scores = []\n","models = []\n","predictions = []\n","callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n","\n","i=1\n","for train_index, test_index in skf.split(X, y_split):\n","   \n","    print('fold',i)\n","    X_train_fold, X_test_fold = X[train_index], X[test_index]\n","    \n","   \n","    y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n","    \n","    model = lgb.LGBMRegressor(\n","                objective = qwk_obj,\n","                metrics = 'None',\n","                learning_rate = 0.05,\n","                max_depth = 5,\n","                num_leaves = 10,\n","                colsample_bytree=0.3,\n","                reg_alpha = 0.7,\n","                reg_lambda = 0.1,\n","                n_estimators=700,\n","                random_state=42,\n","                extra_trees=True,\n","                class_weight='balanced',\n","                verbosity = - 1)\n","\n","    predictor = model.fit(X_train_fold,\n","                                  y_train_fold,\n","                                  eval_names=['train', 'valid'],\n","                                  eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n","                                  eval_metric=quadratic_weighted_kappa,\n","                                  callbacks=callbacks,)\n","    models.append(predictor)\n","    predictions_fold = predictor.predict(X_test_fold)\n","    predictions_fold = predictions_fold + a\n","    oof[test_index]=predictions_fold\n","    predictions_fold = predictions_fold.clip(1, 6).round()\n","    predictions.append(predictions_fold)\n","    f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n","    f1_scores.append(f1_fold)\n","    \n","    \n","    kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n","    kappa_scores.append(kappa_fold)\n","    \n","    cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n","\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                                  display_labels=[x for x in range(1,7)])\n","    disp.plot()\n","    plt.show()\n","    print(f'F1 score across fold: {f1_fold}')\n","    print(f'Cohen kappa score across fold: {kappa_fold}')\n","    i+=1\n","\n","mean_f1_score = np.mean(f1_scores)\n","mean_kappa_score = np.mean(kappa_scores)\n","\n","print(\"=\"*50)\n","print(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\n","print(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')\n","print(\"=\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import pickle\n","\n","# with open('models.pkl', 'wb') as f:\n","#     pickle.dump(models, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# with open('models.pkl', 'rb') as f:\n","#     models = pickle.load(f)"]},{"cell_type":"markdown","metadata":{},"source":["# **Inference**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Paragraph\n","tmp = Paragraph_Preprocess(test)\n","test_feats = Paragraph_Eng(tmp)\n","# Sentence\n","tmp = Sentence_Preprocess(test)\n","test_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n","# Word\n","tmp = Word_Preprocess(test)\n","test_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n","\n","# Tfidf\n","test_tfid = vectorizer.transform([i for i in test['full_text']])\n","dense_matrix = test_tfid.toarray()\n","df = pd.DataFrame(dense_matrix)\n","tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n","df.columns = tfid_columns\n","df['essay_id'] = test_feats['essay_id']\n","test_feats = test_feats.merge(df, on='essay_id', how='left')\n","\n","# CountVectorizer\n","test_tfid = vectorizer_cnt.transform([i for i in test['full_text']])\n","dense_matrix = test_tfid.toarray()\n","df = pd.DataFrame(dense_matrix)\n","tfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\n","df.columns = tfid_columns\n","df['essay_id'] = test_feats['essay_id']\n","test_feats = test_feats.merge(df, on='essay_id', how='left')\n","\n","for i in range(6):\n","    test_feats[f'deberta_oof_{i}'] = predicted_score[:, i]\n","\n","# Features number\n","feature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\n","print('Features number: ',len(feature_names))\n","test_feats.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["probabilities = []\n","for model in models:\n","    proba= model.predict(test_feats[feature_select])+ a\n","    probabilities.append(proba)\n","\n","predictions = np.mean(probabilities, axis=0)\n","\n","predictions = np.round(predictions.clip(1, 6))\n","\n","print(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\n","submission['score']=predictions\n","submission['score']=submission['score'].astype(int)\n","submission.to_csv(\"submission.csv\",index=None)\n","display(submission.head())"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8059942,"sourceId":71485,"sourceType":"competition"},{"datasetId":4813598,"sourceId":8141507,"sourceType":"datasetVersion"},{"datasetId":4832208,"sourceId":8166166,"sourceType":"datasetVersion"},{"datasetId":4902588,"sourceId":8260229,"sourceType":"datasetVersion"},{"sourceId":170434135,"sourceType":"kernelVersion"},{"sourceId":170531930,"sourceType":"kernelVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
